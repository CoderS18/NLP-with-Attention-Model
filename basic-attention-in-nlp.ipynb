{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-07T17:47:05.866538Z","iopub.execute_input":"2024-07-07T17:47:05.866976Z","iopub.status.idle":"2024-07-07T17:47:06.208207Z","shell.execute_reply.started":"2024-07-07T17:47:05.866951Z","shell.execute_reply":"2024-07-07T17:47:06.207075Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"hidden_size = 16\nattention_size = 10\ninput_length = 5\n\nnp.random.seed(42)\n\n# Synthetic vectors used to test\nencoder_states = np.random.randn(input_length, hidden_size)\ndecoder_state = np.random.randn(1, hidden_size)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T18:12:33.032216Z","iopub.execute_input":"2024-07-07T18:12:33.032511Z","iopub.status.idle":"2024-07-07T18:12:33.040996Z","shell.execute_reply.started":"2024-07-07T18:12:33.032491Z","shell.execute_reply":"2024-07-07T18:12:33.039907Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#dimention (5,16)\nencoder_states","metadata":{"execution":{"iopub.status.busy":"2024-07-07T18:12:35.274158Z","iopub.execute_input":"2024-07-07T18:12:35.274505Z","iopub.status.idle":"2024-07-07T18:12:35.282146Z","shell.execute_reply.started":"2024-07-07T18:12:35.274484Z","shell.execute_reply":"2024-07-07T18:12:35.280934Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([[ 0.49671415, -0.1382643 ,  0.64768854,  1.52302986, -0.23415337,\n        -0.23413696,  1.57921282,  0.76743473, -0.46947439,  0.54256004,\n        -0.46341769, -0.46572975,  0.24196227, -1.91328024, -1.72491783,\n        -0.56228753],\n       [-1.01283112,  0.31424733, -0.90802408, -1.4123037 ,  1.46564877,\n        -0.2257763 ,  0.0675282 , -1.42474819, -0.54438272,  0.11092259,\n        -1.15099358,  0.37569802, -0.60063869, -0.29169375, -0.60170661,\n         1.85227818],\n       [-0.01349722, -1.05771093,  0.82254491, -1.22084365,  0.2088636 ,\n        -1.95967012, -1.32818605,  0.19686124,  0.73846658,  0.17136828,\n        -0.11564828, -0.3011037 , -1.47852199, -0.71984421, -0.46063877,\n         1.05712223],\n       [ 0.34361829, -1.76304016,  0.32408397, -0.38508228, -0.676922  ,\n         0.61167629,  1.03099952,  0.93128012, -0.83921752, -0.30921238,\n         0.33126343,  0.97554513, -0.47917424, -0.18565898, -1.10633497,\n        -1.19620662],\n       [ 0.81252582,  1.35624003, -0.07201012,  1.0035329 ,  0.36163603,\n        -0.64511975,  0.36139561,  1.53803657, -0.03582604,  1.56464366,\n        -2.6197451 ,  0.8219025 ,  0.08704707, -0.29900735,  0.09176078,\n        -1.98756891]])"},"metadata":{}}]},{"cell_type":"code","source":"decoder_state","metadata":{"execution":{"iopub.status.busy":"2024-07-07T18:12:38.242214Z","iopub.execute_input":"2024-07-07T18:12:38.242689Z","iopub.status.idle":"2024-07-07T18:12:38.253495Z","shell.execute_reply.started":"2024-07-07T18:12:38.242655Z","shell.execute_reply":"2024-07-07T18:12:38.251655Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([[-0.21967189,  0.35711257,  1.47789404, -0.51827022, -0.8084936 ,\n        -0.50175704,  0.91540212,  0.32875111, -0.5297602 ,  0.51326743,\n         0.09707755,  0.96864499, -0.70205309, -0.32766215, -0.39210815,\n        -1.46351495]])"},"metadata":{}}]},{"cell_type":"code","source":"repeated_decoder_state = np.repeat(decoder_state, input_length, axis=0)\nrepeated_decoder_state","metadata":{"execution":{"iopub.status.busy":"2024-07-07T18:12:40.405557Z","iopub.execute_input":"2024-07-07T18:12:40.407252Z","iopub.status.idle":"2024-07-07T18:12:40.415643Z","shell.execute_reply.started":"2024-07-07T18:12:40.407219Z","shell.execute_reply":"2024-07-07T18:12:40.414476Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([[-0.21967189,  0.35711257,  1.47789404, -0.51827022, -0.8084936 ,\n        -0.50175704,  0.91540212,  0.32875111, -0.5297602 ,  0.51326743,\n         0.09707755,  0.96864499, -0.70205309, -0.32766215, -0.39210815,\n        -1.46351495],\n       [-0.21967189,  0.35711257,  1.47789404, -0.51827022, -0.8084936 ,\n        -0.50175704,  0.91540212,  0.32875111, -0.5297602 ,  0.51326743,\n         0.09707755,  0.96864499, -0.70205309, -0.32766215, -0.39210815,\n        -1.46351495],\n       [-0.21967189,  0.35711257,  1.47789404, -0.51827022, -0.8084936 ,\n        -0.50175704,  0.91540212,  0.32875111, -0.5297602 ,  0.51326743,\n         0.09707755,  0.96864499, -0.70205309, -0.32766215, -0.39210815,\n        -1.46351495],\n       [-0.21967189,  0.35711257,  1.47789404, -0.51827022, -0.8084936 ,\n        -0.50175704,  0.91540212,  0.32875111, -0.5297602 ,  0.51326743,\n         0.09707755,  0.96864499, -0.70205309, -0.32766215, -0.39210815,\n        -1.46351495],\n       [-0.21967189,  0.35711257,  1.47789404, -0.51827022, -0.8084936 ,\n        -0.50175704,  0.91540212,  0.32875111, -0.5297602 ,  0.51326743,\n         0.09707755,  0.96864499, -0.70205309, -0.32766215, -0.39210815,\n        -1.46351495]])"},"metadata":{}}]},{"cell_type":"code","source":"# Weights for the neural network, these are typically learned through training\n# Use these in the alignment function below as the layer weights\nlayer_1 = np.random.randn(2 * hidden_size, attention_size)\nlayer_2 = np.random.randn(attention_size, 1)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T18:12:43.531511Z","iopub.execute_input":"2024-07-07T18:12:43.531865Z","iopub.status.idle":"2024-07-07T18:12:43.537214Z","shell.execute_reply.started":"2024-07-07T18:12:43.531843Z","shell.execute_reply":"2024-07-07T18:12:43.536068Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"layer_2","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:52:10.218350Z","iopub.execute_input":"2024-07-07T17:52:10.218737Z","iopub.status.idle":"2024-07-07T17:52:10.225769Z","shell.execute_reply.started":"2024-07-07T17:52:10.218709Z","shell.execute_reply":"2024-07-07T17:52:10.224848Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([[ 1.55115198],\n       [ 0.11567463],\n       [ 1.17929718],\n       [ 0.06751848],\n       [ 2.06074792],\n       [ 1.75534084],\n       [-0.24896415],\n       [ 0.97157095],\n       [ 0.64537595],\n       [ 1.36863156]])"},"metadata":{}}]},{"cell_type":"code","source":"def alignment(encoder_states, decoder_state):\n    # First, concatenate the encoder states and the decoder state.\n    inputs = np.concatenate((encoder_states, decoder_state), axis=1)\n    print(inputs.shape)\n    assert inputs.shape == (input_length, 2*hidden_size)\n    \n    # Matrix multiplication of the concatenated inputs and the first layer, with tanh activation\n    activations = np.tanh(np.matmul(inputs, layer_1))\n    assert activations.shape == (input_length, attention_size)\n    \n    # Matrix multiplication of the activations with the second layer. Remember that you don't need tanh here\n    scores = np.matmul(activations, layer_2)\n    assert scores.shape == (input_length, 1)\n    \n    return scores","metadata":{"execution":{"iopub.status.busy":"2024-07-07T18:12:47.004918Z","iopub.execute_input":"2024-07-07T18:12:47.005362Z","iopub.status.idle":"2024-07-07T18:12:47.013598Z","shell.execute_reply.started":"2024-07-07T18:12:47.005337Z","shell.execute_reply":"2024-07-07T18:12:47.012388Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Run this to test your alignment function\nscores = alignment(encoder_states, repeated_decoder_state)\nprint(scores)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T18:12:48.882832Z","iopub.execute_input":"2024-07-07T18:12:48.884124Z","iopub.status.idle":"2024-07-07T18:12:48.894025Z","shell.execute_reply.started":"2024-07-07T18:12:48.884095Z","shell.execute_reply":"2024-07-07T18:12:48.892924Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(5, 32)\n[[4.35790943]\n [5.92373433]\n [4.18673175]\n [2.11437202]\n [0.95767155]]\n","output_type":"stream"}]},{"cell_type":"code","source":"def softmax(x, axis=0):\n    \"\"\" Calculate softmax function for an array x along specified axis\n    \n        axis=0 calculates softmax across rows which means each column sums to 1 \n        axis=1 calculates softmax across columns which means each row sums to 1\n    \"\"\"\n    return np.exp(x) / np.expand_dims(np.sum(np.exp(x), axis=axis), axis)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T18:22:56.547832Z","iopub.execute_input":"2024-07-07T18:22:56.548164Z","iopub.status.idle":"2024-07-07T18:22:56.553524Z","shell.execute_reply.started":"2024-07-07T18:22:56.548143Z","shell.execute_reply":"2024-07-07T18:22:56.552796Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def attention(encoder_states, decoder_state):\n    \"\"\" Example function that calculates attention, returns the context vector \n    \n        Arguments:\n        encoder_vectors: NxM numpy array, where N is the number of vectors and M is the vector length\n        decoder_vector: 1xM numpy array, M is the vector length, much be the same M as encoder_vectors\n    \"\"\" \n    \n    # First, calculate the dot product of each encoder vector with the decoder vector\n    scores = alignment(encoder_states, decoder_state)\n    \n    # Then take the softmax of those scores to get a weight distribution\n    weights = softmax(scores)\n    \n    # Multiply each encoder state by its respective weight\n    weighted_scores = encoder_states * weights\n    \n    # Sum up the weights encoder states\n    context = np.sum(weighted_scores, axis=0)\n    \n    return context\n\ncontext_vector = attention(encoder_states, repeated_decoder_state)\nprint(context_vector)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T18:23:26.513387Z","iopub.execute_input":"2024-07-07T18:23:26.513778Z","iopub.status.idle":"2024-07-07T18:23:26.522517Z","shell.execute_reply.started":"2024-07-07T18:23:26.513754Z","shell.execute_reply":"2024-07-07T18:23:26.521042Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"(5, 32)\n[-0.63514569  0.04917298 -0.43930867 -0.9268003   1.01903919 -0.43181409\n  0.13365099 -0.84746874 -0.37572203  0.18279832 -0.90452701  0.17872958\n -0.58015282 -0.58294027 -0.75457577  1.32985756]\n","output_type":"stream"}]}]}